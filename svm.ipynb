{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbpJXlg7u0ju"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "1. What is a Support Vector Machine (SVM)?\n",
        "   - A supervised ML algorithm that finds the hyperplane separating classes with the largest margin.\n",
        "\n",
        "2. What is the difference between Hard Margin and Soft Margin SVM?\n",
        "   - Hard Margin: Perfect separation, no errors allowed (requires linearly separable data).\n",
        "   - Soft Margin: Allows misclassification for better generalization.\n",
        "\n",
        "3. What is the mathematical intuition behind SVM?\n",
        "   - Maximize the margin between classes while minimizing classification error.\n",
        "\n",
        "4. What is the role of Lagrange Multipliers in SVM?\n",
        "   - Solve the constrained optimization problem for finding the optimal hyperplane.\n",
        "\n",
        "5. What are Support Vectors in SVM?\n",
        "   - Data points closest to the decision boundary that define its position.\n",
        "\n",
        "6. What is a Support Vector Classifier (SVC)?\n",
        "   - Classification variant of SVM.\n",
        "\n",
        "7. What is a Support Vector Regressor (SVR)?\n",
        "   - Regression variant of SVM that predicts continuous values using an ε-insensitive margin.\n",
        "\n",
        "8. What is the Kernel Trick in SVM?\n",
        "   - Maps data into higher-dimensional space to make it linearly separable without explicit computation.\n",
        "\n",
        "9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel:\n",
        "   - Linear: Best for linearly separable data.\n",
        "   - Polynomial: Captures polynomial relationships.\n",
        "   - RBF: Captures complex nonlinear patterns via Gaussian similarity.\n",
        "\n",
        "10. What is the effect of the C parameter in SVM?\n",
        "    - Controls trade-off between margin size and classification error (low C = wider margin, high C = fewer errors).\n",
        "\n",
        "11. What is the role of the Gamma parameter in RBF Kernel SVM?\n",
        "    - Determines influence of a single training point (low gamma = far reach, high gamma = close reach).\n",
        "\n",
        "12. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "    - Probabilistic classifier assuming feature independence; \"naïve\" due to unrealistic independence assumption.\n",
        "\n",
        "13. What is Bayes’ Theorem?\n",
        "    - P(A|B) = [P(B|A) * P(A)] / P(B)\n",
        "\n",
        "14. Explain the differences between Gaussian, Multinomial, and Bernoulli Naïve Bayes:\n",
        "    - Gaussian: Continuous, normally distributed features.\n",
        "    - Multinomial: Discrete counts (e.g., word counts).\n",
        "    - Bernoulli: Binary/boolean features.\n",
        "\n",
        "15. When should you use Gaussian Naïve Bayes over other variants?\n",
        "    - When features are continuous and approximately Gaussian.\n",
        "\n",
        "16. What are the key assumptions made by Naïve Bayes?\n",
        "    - Features are conditionally independent given the class.\n",
        "\n",
        "17. What are the advantages and disadvantages of Naïve Bayes?\n",
        "    - Advantages: Fast, works with small data, good for high-dimensional text.\n",
        "    - Disadvantages: Independence assumption often violated.\n",
        "\n",
        "18. Why is Naïve Bayes a good choice for text classification?\n",
        "    - Word frequencies are approximately independent given the class; works well on sparse high-dimensional data.\n",
        "\n",
        "19. Compare SVM and Naïve Bayes for classification tasks:\n",
        "    - SVM: Margin-based, good for complex boundaries, slower on very large data.\n",
        "    - NB: Probabilistic, faster for large text datasets, less effective for complex boundaries.\n",
        "\n",
        "20. How does Laplace Smoothing help in Naïve Bayes?\n",
        "    - Adds a constant (usually 1) to counts to prevent zero probability issues.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 1 (#H): Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy\n",
        "# ===============================\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "clf = SVC()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(f\"1 (#H) Iris SVM Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "# ===============================\n",
        "# 2 (##): Train two SVM classifiers with Linear and RBF kernels on Wine dataset, compare accuracies\n",
        "# ===============================\n",
        "wine = datasets.load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3, random_state=42)\n",
        "clf_linear = SVC(kernel='linear')\n",
        "clf_rbf = SVC(kernel='rbf')\n",
        "clf_linear.fit(X_train, y_train)\n",
        "clf_rbf.fit(X_train, y_train)\n",
        "acc_linear = accuracy_score(y_test, clf_linear.predict(X_test))\n",
        "acc_rbf = accuracy_score(y_test, clf_rbf.predict(X_test))\n",
        "print(f\"2 (##) Wine SVM Linear Accuracy: {acc_linear:.4f}, RBF Accuracy: {acc_rbf:.4f}\")\n",
        "\n",
        "# ===============================\n",
        "# 3 (#$): SVM Regressor (SVR) on California housing dataset, evaluate with MSE\n",
        "# ===============================\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, test_size=0.3, random_state=42)\n",
        "svr = SVR(kernel='rbf')\n",
        "svr.fit(X_train, y_train)\n",
        "y_pred = svr.predict(X_test)\n",
        "print(f\"3 (#$) SVR Housing MSE: {mean_squared_error(y_test, y_pred):.4f}\")\n",
        "\n",
        "# ===============================\n",
        "# 4 (#%): SVM with Polynomial Kernel, visualize decision boundary\n",
        "# ===============================\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
        "                           random_state=42, n_clusters_per_class=1)\n",
        "clf_poly = SVC(kernel='poly', degree=3)\n",
        "clf_poly.fit(X, y)\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"4 (#%) SVM Polynomial Kernel Decision Boundary\")\n",
        "xx, yy = np.meshgrid(np.linspace(X[:,0].min()-1, X[:,0].max()+1, 200),\n",
        "                     np.linspace(X[:,1].min()-1, X[:,1].max()+1, 200))\n",
        "Z = clf_poly.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "plt.scatter(X[:,0], X[:,1], c=y, edgecolor='k')\n",
        "plt.show()\n",
        "\n",
        "# ===============================\n",
        "# 5 (#!): Gaussian Naïve Bayes on Breast Cancer dataset, evaluate accuracy\n",
        "# ===============================\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.3, random_state=42)\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred = gnb.predict(X_test)\n",
        "print(f\"5 (#!) GaussianNB Breast Cancer Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "# ===============================\n",
        "# 6 (#;): Multinomial Naïve Bayes for text classification (20 Newsgroups)\n",
        "# ===============================\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "newsgroups = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_vec = vectorizer.fit_transform(newsgroups.data)\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_vec, newsgroups.target)\n",
        "acc = mnb.score(X_vec, newsgroups.target)\n",
        "print(f\"6 (#;) MultinomialNB 20 Newsgroups Training Accuracy: {acc:.4f}\")\n",
        "\n",
        "# ===============================\n",
        "# 7 (\u001c?=): SVM with different C values, visualize decision boundaries\n",
        "# ===============================\n",
        "C_values = [0.1, 1, 10]\n",
        "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
        "                           random_state=42, n_clusters_per_class=1)\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "for i, C in enumerate(C_values, 1):\n",
        "    plt.subplot(1, 3, i)\n",
        "    clf_c = SVC(C=C, kernel='linear')\n",
        "    clf_c.fit(X, y)\n",
        "    xx, yy = np.meshgrid(np.linspace(X[:,0].min()-1, X[:,0].max()+1, 200),\n",
        "                         np.linspace(X[:,1].min()-1, X[:,1].max()+1, 200))\n",
        "    Z = clf_c.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "    plt.scatter(X[:,0], X[:,1], c=y, edgecolor='k')\n",
        "    plt.title(f\"C={C}\")\n",
        "plt.suptitle(\"7 (\u001c?=) SVM Decision Boundaries with Different C Values\")\n",
        "plt.show()\n",
        "\n",
        "# ===============================\n",
        "# 8 (\u001c%=): Bernoulli Naïve Bayes for binary classification\n",
        "# ===============================\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "X_bin = np.random.randint(0, 2, size=(100, 5))\n",
        "y_bin = np.random.randint(0, 2, size=(100,))\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_bin, y_bin)\n",
        "print(f\"8 (\u001c%=) BernoulliNB Accuracy: {bnb.score(X_bin, y_bin):.4f}\")\n",
        "\n",
        "# ===============================\n",
        "# 9 (\u001c\u001b=): Apply feature scaling before training SVM, compare with unscaled\n",
        "# ===============================\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X, y = make_classification(n_samples=200, n_features=5, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "clf_unscaled = SVC()\n",
        "clf_unscaled.fit(X_train, y_train)\n",
        "acc_unscaled = clf_unscaled.score(X_test, y_test)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "clf_scaled = SVC()\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "acc_scaled = clf_scaled.score(X_test_scaled, y_test)\n",
        "\n",
        "print(f\"9 (\u001c\u001b=) Unscaled Accuracy: {acc_unscaled:.4f}, Scaled Accuracy: {acc_scaled:.4f}\")\n",
        "\n",
        "# ===============================\n",
        "# 10 (\u001d*=): GaussianNB predictions before & after Laplace smoothing\n",
        "# ===============================\n",
        "gnb = GaussianNB(var_smoothing=1e-9)\n",
        "gnb.fit(X_train, y_train)\n",
        "pred_before = gnb.score(X_test, y_test)\n",
        "gnb_smooth = GaussianNB(var_smoothing=1e-2)\n",
        "gnb_smooth.fit(X_train, y_train)\n",
        "pred_after = gnb_smooth.score(X_test, y_test)\n",
        "print(f\"10 (\u001d*=) Accuracy before smoothing: {pred_before:.4f}, after smoothing: {pred_after:.4f}\")\n",
        "\n",
        "# ===============================\n",
        "# 11 (\u001d\u0017=): SVM + GridSearchCV to tune C, gamma, kernel\n",
        "# ===============================\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 0.1, 1], 'kernel': ['linear', 'rbf']}\n",
        "grid = GridSearchCV(SVC(), param_grid, cv=3)\n",
        "grid.fit(X_train_scaled, y_train)\n",
        "print(f\"11 (\u001d\u0017=) Best Params: {grid.best_params_}, Best Score: {grid.best_score_:.4f}\")\n",
        "\n",
        "# ===============================\n",
        "# 12 (\u001d\u001c=): SVM on imbalanced dataset with class_weight\n",
        "# ===============================\n",
        "from sklearn.datasets import make_classification\n",
        "X_imb, y_imb = make_classification(n_classes=2, class_sep=2, weights=[0.9, 0.1],\n",
        "                                   n_informative=3, n_redundant=0, flip_y=0,\n",
        "                                   n_features=5, n_clusters_per_class=1, n_samples=200, random_state=10)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_imb, y_imb, random_state=42)\n",
        "clf_no_weight = SVC()\n",
        "clf_weight = SVC(class_weight='balanced')\n",
        "clf_no_weight.fit(X_train, y_train)\n",
        "clf_weight.fit(X_train, y_train)\n",
        "print(f\"12 (\u001d\u001c=) No weight Acc: {clf_no_weight.score(X_test, y_test):.4f}, Weighted Acc: {clf_weight.score(X_test, y_test):.4f}\")\n",
        "\n",
        "# ===============================\n",
        "# 13 (\u001d\u001d=): Naïve Bayes spam detection (synthetic example)\n",
        "# ===============================\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "emails = [\"Win money now\", \"Cheap meds online\", \"Hi friend how are you\", \"Meeting tomorrow at office\"]\n",
        "labels = [1, 1, 0, 0]  # 1 = spam, 0 = ham\n",
        "vectorizer = CountVectorizer()\n",
        "X_vec = vectorizer.fit_transform(emails)\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_vec, labels)\n",
        "print(f\"13 (\u001d\u001d=) Spam Detection Predictions: {mnb.predict(vectorizer.transform(['Win a free iPhone', 'See you at lunch']))}\")\n",
        "\n",
        "# ===============================\n",
        "# 14 (\u001d\u001e=): SVM vs Naïve Bayes on same dataset\n",
        "# ===============================\n",
        "X, y = make_classification(n_samples=300, n_features=10, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "svm_model = SVC()\n",
        "nb_model = GaussianNB()\n",
        "svm_model.fit(X_train, y_train)\n",
        "nb_model.fit(X_train, y_train)\n",
        "print(f\"14 (\u001d\u001e=) SVM Acc: {svm_model.score(X_test, y_test):.4f}, Naïve Bayes Acc: {nb_model.score(X_test, y_test):.4f}\")\n"
      ],
      "metadata": {
        "id": "eGc5TTN8vFwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 15 (\u001d>=): Perform feature selection before Naïve Bayes and compare results\n",
        "# ===============================\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "X, y = make_classification(n_samples=300, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "acc_before = nb_model.score(X_test, y_test)\n",
        "\n",
        "selector = SelectKBest(score_func=f_classif, k=10)\n",
        "X_train_sel = selector.fit_transform(X_train, y_train)\n",
        "X_test_sel = selector.transform(X_test)\n",
        "nb_model_sel = GaussianNB()\n",
        "nb_model_sel.fit(X_train_sel, y_train)\n",
        "acc_after = nb_model_sel.score(X_test_sel, y_test)\n",
        "print(f\"15 (\u001d>=) NB Accuracy before FS: {acc_before:.4f}, after FS: {acc_after:.4f}\")\n",
        "\n",
        "# ===============================\n",
        "# 16 (\u001d<=): SVM OvR vs OvO on Wine dataset\n",
        "# ===============================\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "\n",
        "wine = datasets.load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, random_state=42)\n",
        "ovr = OneVsRestClassifier(SVC())\n",
        "ovo = OneVsOneClassifier(SVC())\n",
        "ovr.fit(X_train, y_train)\n",
        "ovo.fit(X_train, y_train)\n",
        "print(f\"16 (\u001d<=) OvR Accuracy: {ovr.score(X_test, y_test):.4f}, OvO Accuracy: {ovo.score(X_test, y_test):.4f}\")\n",
        "\n",
        "# ===============================\n",
        "# 17 (\u001d?=): SVM Linear, Poly, RBF on Breast Cancer dataset\n",
        "# ===============================\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=42)\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "for k in kernels:\n",
        "    model = SVC(kernel=k)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f\"17 (\u001d?=) Kernel={k} Accuracy: {model.score(X_test, y_test):.4f}\")\n",
        "\n",
        "# ===============================\n",
        "# 18 (\u001d%=): SVM with Stratified K-Fold CV\n",
        "# ===============================\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "scores = cross_val_score(SVC(), cancer.data, cancer.target, cv=skf)\n",
        "print(f\"18 (\u001d%=) Stratified K-Fold Avg Accuracy: {scores.mean():.4f}\")\n",
        "\n",
        "# ===============================\n",
        "# 19 (\u001d\u001b=): Naïve Bayes with different priors\n",
        "# ===============================\n",
        "priors_list = [[0.3, 0.7], [0.5, 0.5], [0.6, 0.4]]\n",
        "for priors in priors_list:\n",
        "    model = GaussianNB(priors=priors)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f\"19 (\u001d\u001b=) Priors={priors}, Accuracy: {model.score(X_test, y_test):.4f}\")\n",
        "\n",
        "# ===============================\n",
        "# 20 (\u001e*=): RFE before SVM and compare accuracy\n",
        "# ===============================\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "X, y = make_classification(n_samples=300, n_features=15, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "svc = SVC(kernel='linear')\n",
        "svc.fit(X_train, y_train)\n",
        "acc_before = svc.score(X_test, y_test)\n",
        "\n",
        "selector = RFE(svc, n_features_to_select=8)\n",
        "selector.fit(X_train, y_train)\n",
        "X_train_sel = selector.transform(X_train)\n",
        "X_test_sel = selector.transform(X_test)\n",
        "svc_sel = SVC(kernel='linear')\n",
        "svc_sel.fit(X_train_sel, y_train)\n",
        "acc_after = svc_sel.score(X_test_sel, y_test)\n",
        "print(f\"20 (\u001e*=) SVM Accuracy before RFE: {acc_before:.4f}, after RFE: {acc_after:.4f}\")\n",
        "\n",
        "# ===============================\n",
        "# 21 (\u001e\u0017=): SVM with Precision, Recall, F1\n",
        "# ===============================\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "model = SVC()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(f\"21 (\u001e\u0017=) Precision: {precision_score(y_test, y_pred):.4f}, Recall: {recall_score(y_test, y_pred):.4f}, F1: {f1_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "# ===============================\n",
        "# 22 (\u001e\u001c=): Naïve Bayes with Log Loss\n",
        "# ===============================\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "y_prob = model.predict_proba(X_test)\n",
        "print(f\"22 (\u001e\u001c=) Log Loss: {log_loss(y_test, y_prob):.4f}\")\n",
        "\n",
        "# ===============================\n",
        "# 23 (\u001e\u001d=): SVM Confusion Matrix with seaborn\n",
        "# ===============================\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "model = SVC()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"23 (\u001e\u001d=) SVM Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n",
        "\n",
        "# ===============================\n",
        "# 24 (\u001e\u001e=): SVR with MAE\n",
        "# ===============================\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
        "svr = SVR()\n",
        "svr.fit(X_train, y_train)\n",
        "y_pred = svr.predict(X_test)\n",
        "print(f\"24 (\u001e\u001e=) SVR MAE: {mean_absolute_error(y_test, y_pred):.4f}\")\n",
        "\n",
        "# ===============================\n",
        "# 25 (\u001e>=): Naïve Bayes with ROC-AUC\n",
        "# ===============================\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=42)\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "print(f\"25 (\u001e>=) ROC-AUC: {roc_auc_score(y_test, y_prob):.4f}\")\n",
        "\n",
        "# ===============================\n",
        "# 26 (\u001e<=): SVM Precision-Recall Curve\n",
        "# ===============================\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "model = SVC(probability=True)\n",
        "model.fit(X_train, y_train)\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.title(\"26 (\u001e<=) SVM Precision-Recall Curve\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bHqwYf_5wzGF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}