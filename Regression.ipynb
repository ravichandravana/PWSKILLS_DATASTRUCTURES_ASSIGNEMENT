{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oefW9up8I2Cz"
      },
      "outputs": [],
      "source": [
        "# 1. What is Simple Linear Regression?\n",
        "# Simple Linear Regression models the relationship between one independent variable (X) and a dependent variable (Y) using Y = mX + c.\n",
        "\n",
        "# 2. What are the key assumptions of Simple Linear Regression?\n",
        "# Linearity, independence of errors, homoscedasticity, normal distribution of residuals, and no multicollinearity.\n",
        "\n",
        "# 3. What does the coefficient m represent in the equation Y = mX + c?\n",
        "# It represents the slope — the change in Y for a one-unit change in X.\n",
        "\n",
        "# 4. What does the intercept c represent in the equation Y = mX + c?\n",
        "# The predicted value of Y when X = 0.\n",
        "\n",
        "# 5. How do we calculate the slope m in Simple Linear Regression?\n",
        "# m = Σ[(Xi - X̄)(Yi - Ȳ)] / Σ[(Xi - X̄)^2]\n",
        "\n",
        "# 6. What is the purpose of the least squares method?\n",
        "# To minimize the sum of the squared differences between actual and predicted values.\n",
        "\n",
        "# 7. How is the coefficient of determination (R²) interpreted?\n",
        "# R² shows the proportion of variance in Y explained by the independent variable(s).\n",
        "\n",
        "# 8. What is Multiple Linear Regression?\n",
        "# A regression model with two or more independent variables predicting one dependent variable.\n",
        "\n",
        "# 9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "# Simple has one independent variable; multiple has more than one.\n",
        "\n",
        "# 10. What are the key assumptions of Multiple Linear Regression?\n",
        "# Linearity, no multicollinearity, homoscedasticity, independence, and normal distribution of errors.\n",
        "\n",
        "# 11. What is heteroscedasticity, and how does it affect regression?\n",
        "# It refers to unequal variance of residuals and can distort statistical tests.\n",
        "\n",
        "# 12. How can you improve a model with high multicollinearity?\n",
        "# Remove or combine correlated features, or use techniques like Ridge or Lasso regression.\n",
        "\n",
        "# 13. What are common techniques for encoding categorical variables?\n",
        "# One-hot encoding, label encoding, and ordinal encoding.\n",
        "\n",
        "# 14. What is the role of interaction terms?\n",
        "# To capture the combined effect of two or more variables on the dependent variable.\n",
        "\n",
        "# 15. How can the intercept interpretation differ between Simple and Multiple Linear Regression?\n",
        "# In multiple regression, it is the predicted Y when all Xs are zero — which may be unrealistic or not meaningful.\n",
        "\n",
        "# 16. What is the significance of the slope in regression?\n",
        "# It tells us how much the target variable changes with a one-unit increase in a predictor variable.\n",
        "\n",
        "# 17. How does the intercept help in understanding variable relationships?\n",
        "# It gives a baseline prediction when all inputs are zero.\n",
        "\n",
        "# 18. What are the limitations of R² as a performance measure?\n",
        "# It does not account for overfitting, model complexity, or whether predictors are meaningful.\n",
        "\n",
        "# 19. How do you interpret a large standard error for a coefficient?\n",
        "# It suggests high variability in the estimate, implying low confidence.\n",
        "\n",
        "# 20. How can heteroscedasticity be detected and why address it?\n",
        "# Detected with residual plots; addressing it is crucial for reliable confidence intervals and p-values.\n",
        "\n",
        "# 21. What does it mean if R² is high but adjusted R² is low?\n",
        "# The model may be overfitting by including irrelevant variables.\n",
        "\n",
        "# 22. Why scale variables in Multiple Linear Regression?\n",
        "# To ensure fair weighting and improve convergence of optimization algorithms.\n",
        "\n",
        "# 23. What is Polynomial Regression?\n",
        "# A regression where the relationship is modeled as an nth-degree polynomial.\n",
        "\n",
        "# 24. How does it differ from linear regression?\n",
        "# Polynomial regression models non-linear (curved) relationships.\n",
        "\n",
        "# 25. When is polynomial regression used?\n",
        "# When the data shows a clear curved trend that a linear model can't capture.\n",
        "\n",
        "# 26. What is the general equation for polynomial regression?\n",
        "# Y = β₀ + β₁X + β₂X² + β₃X³ + ... + βnXⁿ\n",
        "\n",
        "# 27. Can polynomial regression be applied to multiple variables?\n",
        "# Yes, resulting in multivariate polynomial regression.\n",
        "\n",
        "# 28. What are the risks of using higher-degree polynomials?\n",
        "# Overfitting, poor generalization, and high variance in predictions.\n",
        "\n",
        "# 29. How can you choose the best polynomial degree?\n",
        "# Use cross-validation and compare performance metrics like RMSE or adjusted R².\n",
        "\n",
        "# 30. How do you detect overfitting in regression?\n",
        "# Training performance is high but test performance is poor.\n",
        "\n",
        "# 31. What is regularization in regression models?\n",
        "# A technique to penalize large coefficients to reduce overfitting (e.g., Ridge, Lasso).\n"
      ]
    }
  ]
}